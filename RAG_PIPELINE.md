# RAG Pipeline Overview - LiveChat AI

## ğŸ“‹ Tá»•ng Quan

Pipeline RAG (Retrieval-Augmented Generation) cá»§a dá»± Ã¡n **livechat-ai-be** Ä‘Æ°á»£c chia thÃ nh 2 giai Ä‘oáº¡n chÃ­nh:

1. **INGEST Pipeline** - Nháº­p vÃ  xá»­ lÃ½ kiáº¿n thá»©c
2. **QUERY Pipeline** - Truy váº¥n vÃ  sinh cÃ¢u tráº£ lá»i

---

## ğŸ”„ INGEST PIPELINE (Indexing Knowledge)

Pipeline nÃ y xá»­ lÃ½ viá»‡c nháº­p tÃ i liá»‡u vÃ o há»‡ thá»‘ng vÃ  biáº¿n chÃºng thÃ nh vectors cÃ³ thá»ƒ tÃ¬m kiáº¿m.

### ğŸ“Š Flow Chart

```mermaid
graph TD
    A[Upload Document] --> B[ExtractionService]
    B --> C[ChunkingService]
    C --> D[EmbeddingService]
    D --> E[VectorDbService]
    E --> F[MongoDB Storage]
    
    B -->|Text Extraction| B1[PDF/DOCX/TXT]
    C -->|Chunking| C1[500 chars/chunk + 20% overlap]
    D -->|Google Embedding| D1[768-dim vectors]
    E -->|Qdrant| E1[Vector Search Index]
    F -->|Metadata| F1[Document + Chunks]
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e8f5e9
    style F fill:#fce4ec
```

### ğŸ§© Components chi tiáº¿t

#### 1. **ExtractionService** 
ğŸ“ Path: [`src/knowledge/extraction.service.ts`](src/knowledge/extraction.service.ts)

**Nhiá»‡m vá»¥**: TrÃ­ch xuáº¥t text tá»« cÃ¡c file upload (PDF, DOCX, TXT)

**CÃ´ng nghá»‡**: 
- `pdf-parse` cho PDF
- `mammoth` cho DOCX  
- `fs` cho TXT

**Input**: File path + file type  
**Output**: Raw text content

---

#### 2. **ChunkingService**
ğŸ“ Path: [`src/knowledge/chunking.service.ts`](src/knowledge/chunking.service.ts)

**Nhiá»‡m vá»¥**: Chia text thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunks) Ä‘á»ƒ embedding

**Chiáº¿n lÆ°á»£c**:
- **Chunk size**: 500 kÃ½ tá»± má»—i chunk
- **Overlap**: 20% (100 kÃ½ tá»±) Ä‘á»ƒ giá»¯ ngá»¯ cáº£nh
- **Natural breaks**: Æ¯u tiÃªn cáº¯t táº¡i cuá»‘i Ä‘oáº¡n vÄƒn, cuá»‘i cÃ¢u, khoáº£ng tráº¯ng

**PhÃ¡t hiá»‡n chunk type**: 
- `title` - tiÃªu Ä‘á»
- `list` - danh sÃ¡ch
- `code` - code block
- `paragraph` - Ä‘oáº¡n vÄƒn thÃ´ng thÆ°á»ng

**Input**: Raw text  
**Output**: Array of `ChunkResult[]` 
```typescript
{
  content: string,
  chunkIndex: number,
  chunkType: string
}
```

---

#### 3. **EmbeddingService**
ğŸ“ Path: [`src/rag/embedding.service.ts`](src/rag/embedding.service.ts)

**Nhiá»‡m vá»¥**: Chuyá»ƒn Ä‘á»•i text chunks thÃ nh vector embeddings

**Model**: Google `text-embedding-004`  
**Dimension**: 768 chiá»u

**Tá»‘i Æ°u**:
- Batch processing: 100 chunks/batch
- Sá»­ dá»¥ng `batchEmbedContents` API cá»§a Gemini

**Methods**:
- `embed(text)` - Embedding Ä‘Æ¡n láº»
- `embedBatch(texts[])` - Embedding hÃ ng loáº¡t (memory efficient)

**Input**: Text string hoáº·c text array  
**Output**: `number[]` hoáº·c `number[][]` (vector embeddings)

---

#### 4. **VectorDbService**
ğŸ“ Path: [`src/rag/vector-db.service.ts`](src/rag/vector-db.service.ts)

**Nhiá»‡m vá»¥**: LÆ°u trá»¯ vÃ  quáº£n lÃ½ vector embeddings trong Qdrant

**Database**: Qdrant Vector Database  
**Distance metric**: Cosine similarity

**Collection Schema**:
```typescript
{
  vectors: {
    size: 768,
    distance: 'Cosine'
  }
}
```

**Payload Schema**:
```typescript
{
  tenantSlug: string,
  documentId: string,
  category: string,
  documentTitle: string,
  content: string,
  chunkIndex: number
}
```

**Methods**:
- `upsertPoints()` - ThÃªm/cáº­p nháº­t vectors
- `search()` - TÃ¬m kiáº¿m vectors tÆ°Æ¡ng tá»±
- `deleteByDocumentId()` - XÃ³a vectors theo document

---

#### 5. **KnowledgeIndexingProcessor**
ğŸ“ Path: [`src/queue/processors/knowledge-indexing.processor.ts`](src/queue/processors/knowledge-indexing.processor.ts)

**Nhiá»‡m vá»¥**: Orchestrator Ä‘iá»u phá»‘i toÃ n bá»™ INGEST pipeline

**Queue**: BullMQ (Redis-backed)  
**Processing**: Batch streaming (15 chunks at a time)

**Pipeline Steps**:
1. Fetch document tá»« MongoDB
2. Extract text (náº¿u chÆ°a cÃ³)
3. Chunk content
4. Process theo batch:
   - Embed batch chunks
   - Prepare Qdrant points
   - Upsert to Qdrant
   - Save chunk metadata to MongoDB
5. Update document status â†’ `indexed`

**Memory Optimization**: 
- O(batch_size) thay vÃ¬ O(total_chunks)
- Streaming pipeline giÃºp xá»­ lÃ½ tÃ i liá»‡u lá»›n mÃ  khÃ´ng trÃ n bá»™ nhá»›

**Status tracking**:
- `pending` â†’ `indexing` â†’ `indexed` / `failed`
- Progress update qua BullMQ job progress

---

## ğŸ” QUERY PIPELINE (Retrieval & Generation)

Pipeline nÃ y xá»­ lÃ½ cÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng vÃ  sinh cÃ¢u tráº£ lá»i thÃ´ng minh.

### ğŸ“Š Flow Chart

```mermaid
graph TD
    A[User Query] --> B[EmbeddingService]
    B --> C[VectorDbService Search]
    C --> D[RagService Generate]
    D --> E[LlmService]
    E --> F[Response + Confidence]
    
    B -->|Query Embedding| B1[768-dim vector]
    C -->|Vector Search| C1[Top-K similar chunks]
    D -->|Build Prompt| D1[System + Context + History]
    E -->|Gemini 2.5 Flash| E1[Chat Completion]
    F -->|Calculate Confidence| F1[Based on scores + response]
    
    style A fill:#e1f5ff
    style B fill:#f3e5f5
    style C fill:#e8f5e9
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#c8e6c9
```

### ğŸ§© Components chi tiáº¿t

#### 1. **RagService.retrieve()**
ğŸ“ Path: [`src/rag/rag.service.ts`](src/rag/rag.service.ts)

**Nhiá»‡m vá»¥**: TÃ¬m kiáº¿n thá»©c liÃªn quan tá»« vector database

**Process**:
1. Embed cÃ¢u há»i thÃ nh vector (via `EmbeddingService`)
2. Search trong Qdrant vá»›i filters:
   - `tenantSlug` (báº¯t buá»™c)
   - `category` (optional)
3. Tráº£ vá» top-K chunks (máº·c Ä‘á»‹nh K=5)

**Input**:
```typescript
{
  query: string,
  tenantSlug: string,
  category?: string,
  topK?: number
}
```

**Output**:
```typescript
{
  chunks: SearchResult[],
  maxScore: number
}
```

---

#### 2. **RagService.generate()**
ğŸ“ Path: [`src/rag/rag.service.ts`](src/rag/rag.service.ts)

**Nhiá»‡m vá»¥**: Táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn context vÃ  lá»‹ch sá»­ há»™i thoáº¡i

**Process**:
1. Build system prompt (theo config: professional/friendly)
2. Build user prompt vá»›i context chunks
3. Káº¿t há»£p conversation history (max 10 messages gáº§n nháº¥t)
4. Gá»i LLM (Gemini 2.5 Flash)
5. TÃ­nh confidence score

**Confidence Calculation**:
- Base: Average score cá»§a top-3 chunks
- Penalty: Náº¿u phÃ¡t hiá»‡n "uncertainty phrases" â†’ confidence *= 0.5
- Uncertainty phrases: "khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin", "khÃ´ng biáº¿t", "khÃ´ng cháº¯c", "nhÃ¢n viÃªn há»— trá»£"

**Input**:
```typescript
{
  query: string,
  context: SearchResult[],
  conversationHistory: Array<{role, content}>,
  config: {
    responseStyle: string,
    maxResponseLength: number,
    language: string,
    aiDisplayName: string
  }
}
```

**Output**:
```typescript
{
  response: string,
  confidence: number,  // 0-1
  tokenUsage: number,
  processingTime: number  // ms
}
```

---

#### 3. **LlmService**
ğŸ“ Path: [`src/rag/llm.service.ts`](src/rag/llm.service.ts)

**Nhiá»‡m vá»¥**: Giao tiáº¿p vá»›i Gemini LLM API

**Model**: `gemini-2.0-flash-exp` (configurable)

**Features**:
- Multi-turn conversation support
- System prompt customization
- Temperature control (máº·c Ä‘á»‹nh 0.3 cho RAG)
- Token usage tracking

**Method**: `chat()`
```typescript
{
  systemPrompt: string,
  messages: Array<{role: 'user'|'assistant', content: string}>,
  temperature?: number,
  maxTokens?: number
}
```

---

## ğŸ“ File Structure Summary

```
livechat-ai-be/src/
â”œâ”€â”€ rag/                          # ğŸ” QUERY Pipeline Core
â”‚   â”œâ”€â”€ embedding.service.ts      # Embedding (cáº£ Ingest & Query)
â”‚   â”œâ”€â”€ vector-db.service.ts      # Vector DB (cáº£ Ingest & Query) 
â”‚   â”œâ”€â”€ llm.service.ts            # LLM Generation
â”‚   â””â”€â”€ rag.service.ts            # RAG Orchestrator (retrieve + generate)
â”‚
â”œâ”€â”€ knowledge/                    # ğŸ“¥ INGEST Pipeline
â”‚   â”œâ”€â”€ extraction.service.ts    # Text extraction
â”‚   â”œâ”€â”€ chunking.service.ts      # Text chunking
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ document.schema.ts   # Document metadata
â”‚       â””â”€â”€ chunk.schema.ts      # Chunk metadata
â”‚
â””â”€â”€ queue/                        # âš™ï¸ Background Processing
    â””â”€â”€ processors/
        â””â”€â”€ knowledge-indexing.processor.ts  # INGEST orchestrator
```

---

## ğŸ¯ Key Design Patterns

### Multi-Tenancy
- Má»i thao tÃ¡c Ä‘á»u filter theo `tenantSlug`
- Isolation hoÃ n toÃ n giá»¯a cÃ¡c tenant trong cÃ¹ng 1 Qdrant collection

### Async Processing
- Upload tÃ i liá»‡u tráº£ vá» ngay (status: `pending`)
- Background job xá»­ lÃ½ indexing
- Client poll status qua API

### Memory Efficiency
- Batch streaming processing (15 chunks/batch)
- KhÃ´ng load toÃ n bá»™ chunks vÃ o memory
- TrÃ¡nh OOM vá»›i tÃ i liá»‡u lá»›n

### Confidence-based Escalation
- Confidence < threshold â†’ escalate to human agent
- Tá»± Ä‘á»™ng phÃ¡t hiá»‡n cÃ¢u tráº£ lá»i khÃ´ng cháº¯c cháº¯n

---

## ğŸ”— Integration Points

### Chat Flow
ğŸ“ [`src/chat/chat.service.ts`](src/chat/chat.service.ts)

```typescript
const retrievalResult = await ragService.retrieve({...});
const ragResult = await ragService.generate({...});

if (ragResult.confidence < config.confidenceThreshold) {
  // Escalate to human agent
}
```

### Knowledge Management
ğŸ“ [`src/knowledge/knowledge.controller.ts`](src/knowledge/knowledge.controller.ts)

```typescript
// Upload document â†’ push to queue
await queue.add('index-document', {...});

// Query knowledge
await ragService.retrieve({...});
```

---

## âœ… Káº¿t Luáº­n

Pipeline RAG hiá»‡n táº¡i Ä‘Ã£ Ä‘Æ°á»£c tá»• chá»©c tÆ°Æ¡ng Ä‘á»‘i rÃµ rÃ ng theo 2 giai Ä‘oáº¡n:

### âœ… INGEST (Indexing)
1. Extraction â†’ 2. Chunking â†’ 3. Embedding â†’ 4. Vector Storage â†’ 5. Metadata Storage

### âœ… QUERY (Retrieval + Generation)  
1. Query Embedding â†’ 2. Vector Search â†’ 3. LLM Generation â†’ 4. Confidence Scoring

Tuy nhiÃªn, cÃ³ thá»ƒ cáº£i thiá»‡n thÃªm vá»:
- **Documentation**: ThÃªm comments vÃ  architectural decision records (ADRs)
- **Monitoring**: Metrics cho tá»«ng bÆ°á»›c trong pipeline
- **Error handling**: Retry logic vÃ  fallback strategies
